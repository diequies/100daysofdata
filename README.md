# **My 1<sup>st</sup> #100DaysOfData Challenge**
In this repository I will track my first challenge of #100daysofdata, using as a summary of my activities, the books I read, the courses I take and the projects I make.

| Projects |
| -------- |
| 1. [**Zoopla London Houses Price Prediction from Scratch**](https://github.com/diequies/zoopla_houses) |
| 2. [**Building A Recommendation System For Cognitive Exercises and Training Programs**](https://omdena.com/projects/cognitive-exercises/) |
| 3. [**Using Computer Vision to Detect Ethnicity in News and Videos and Improve Ethnicity Awareness**](https://omdena.com/projects/ethnicity-awareness/)

| Courses and Certifications |
| ------------------------- |
| 1. [**2021 Python for Machine Learning & Data Science**](https://www.udemy.com/course/python-for-machine-learning-data-science-masterclass/) |
| 2. [**Python Programming datacamp Skill Track**](https://learn.datacamp.com/skill-tracks/python-programming?version=2) |
| 3. [**Machine Learning by Andrew Ng and Stanford University**](https://www.coursera.org/learn/machine-learning/home) |
| 4. [**Machine Learning Fundamentals datacamp Skill Track**](https://app.datacamp.com/learn/skill-tracks/machine-learning-fundamentals-with-python?version=1) |
| 5. [**Deep Learning Specialization**](https://www.coursera.org/specializations/deep-learning?=&page=1) |
| 6. [**Machine Learning Scientist with Python datacamp Career Track**](https://app.datacamp.com/learn/career-tracks/machine-learning-scientist-with-python) |

| Books and Papers |
| ---------------- |
| 1. [**Python Data Science Handbook**](https://jakevdp.github.io/PythonDataScienceHandbook/) |
| 2. [**Machine Learning Yearning**](https://github.com/ajaymache/machine-learning-yearning) |
| 3. [**Hands-on Machine Learning with Scikit-Learn, Keras & TensorFlow**](https://github.com/ageron/handson-ml2) |

---

### **Days 116 of #100DaysOfData Challenge**

Continued focused on learning Active Learning for *Omdena* and the Deep Learning Specialization at Coursera.

1. **Project**: [Using Computer Vision to Detect Ethnicity in News and Videos and Improve Ethnicity Awareness](https://omdena.com/projects/ethnicity-awareness/). I kept reading the book *Human-in-the-loop Machine Learning* getting very useful new knowledge, I will need to summarize it to share it to the team. Following this new insights our strategy will change and the process flow gets more clear. Now I know that we need 4 validation sets instead of 4 and also that we should get the activation information from all the layers.
2. **Course**: [Deep Learning Specialization](https://www.coursera.org/specializations/deep-learning?=&page=1). Completed the first two weeks of the second course *Improving Deep Neural Networks: Hyperparameter Tuning, Regularization and Optimization, the title is self-explanatory*. The first week was about the over-fitting problem and regularization techniques, such as L2 regularization or dropout and optimization technicques, like weights initialization and normalizing inputs. The second week moved to optimization algorithms, slowly introducing Adam and learning rate decay.

---

### **Days 115 of #100DaysOfData Challenge**

Important day in our *Omdena* project, we had our brainstorming session and also I could finish the first course of Deep Learning specialization in Coursera. 

1. **Project**: [Using Computer Vision to Detect Ethnicity in News and Videos and Improve Ethnicity Awareness](https://omdena.com/projects/ethnicity-awareness/). The purpose of the brainstorming session was to draft the *Active Learning* tool structure we want to build. We focused on settle its characteristics and requirements and now we have to create a clear flow to start coding. Until Friday I'll keep reading the book *Human-in-the-loop in Machine Learning* as it the main source of knowledge.
2. **Course**: [Deep Learning Specialization](https://www.coursera.org/specializations/deep-learning?=&page=1). Finished the last week and therefore the 1st course of *Deep Learning* specialization. In this last week we built a Deep Neural Network and compared to the shallow one to understand the improvements, and problems, of having a more complex *NN*. Now I am starting the next course, *Improving Deep Neural Networks: Hyperparameter Tuning, Regularization and Optimization, the title is self-explanatory*.

---

### **Days 114 of #100DaysOfData Challenge**

Today I focused on Deep Learning Specialization from *Coursera*, I wanted to finish the firs course to meet the objective to complete the course in a month. Also I spent some time reading an Active learning book for *Omdena* project.

1. **Project**: [Using Computer Vision to Detect Ethnicity in News and Videos and Improve Ethnicity Awareness](https://omdena.com/projects/ethnicity-awareness/). I've been reading the book *Human-in-the-loop Machine Learning*, the author does exactly what we want, so the book is giving me unvaluable insights. Tomorrow we have the brainstorming session to define the tool structure, probably one of the milestones.
2. **Course**: [Deep Learning Specialization](https://www.coursera.org/specializations/deep-learning?=&page=1). Completed the first course, *Neural Networks and Deep Learning*. The course finished implementing from scractch a deep neural network. It is really good, it gives you the foundations to move forwards. Also, it is important that they don't use frameworks like TensorFlow yet, so we get a better understanding.

---

### **Days 113 of #100DaysOfData Challenge**

Learning session in our *Omdena* project and some more progress on my courses. Currently I am focused on get ahead on my courses to focus on *Omdena* project later.

1. **Project**: [Using Computer Vision to Detect Ethnicity in News and Videos and Improve Ethnicity Awareness](https://omdena.com/projects/ethnicity-awareness/). In today's learning session I got my first contact with *Pytorch*, it seems complicated but still everything made sense to me. I need to give it a go and try to do an implementation by myself.
2. **Course**: [Deep Learning Specialization](https://www.coursera.org/specializations/deep-learning?=&page=1). Finished the 3rd week of the first course. This time we coded I whole neural network of 2 layers with two different activation functions, *tanh* and *sigmoid*. It seems not so complicated, although I suppose it can get much more. Moving on to the last week, we will code a deeper neural network.

---

### **Days 112 of #100DaysOfData Challenge**

Kind of relaxing day, just attended some meetings and progressed a bit the courses.

1. **Project**: [Using Computer Vision to Detect Ethnicity in News and Videos and Improve Ethnicity Awareness](https://omdena.com/projects/ethnicity-awareness/). Just attended the weekly meeting and explained data labeling progress. Also attended face detection meeting, I will try to optimize the speed for *RetinaFace* algorithm.
2. **Course**: [Machine Learning Scientist with Python datacamp Career Track](https://app.datacamp.com/learn/career-tracks/machine-learning-scientist-with-python). Finished tree based classifiers course. Again, there is nothing new but it is a good refresher of the theory and also a bit of practice. The next module is about *XGBoost* a bit newer for me.

---

### **Days 111 of #100DaysOfData Challenge**

Not the most succesfull day, just kept trying different setups for the *Omdena* CNN and progressed a bit in the courses and book.

1. **Project**: [Using Computer Vision to Detect Ethnicity in News and Videos and Improve Ethnicity Awareness](https://omdena.com/projects/ethnicity-awareness/). Not much time spent on the project today, just some to progress the model and also a quick discussion to get help with it.
2. **Course**: [Deep Learning Specialization](https://www.coursera.org/specializations/deep-learning?=&page=1). Finished the 2nd week of the 1st course, it was focused on how a neurone works in a neural network and how to compute it mathematically and using sigmoid as activation function. The course is really insightful and goes deep enough to get a good understanding of what you are doing. During the 3rd week we move to build a shallow neural network.

---

### **Days 110 of #100DaysOfData Challenge**

During today I tried to progress mainly on my courses and books, although also I kept up with the model training for *Omdena*.

1. **Project**: [Using Computer Vision to Detect Ethnicity in News and Videos and Improve Ethnicity Awareness](https://omdena.com/projects/ethnicity-awareness/). Even though I am being completely unsuccesful because of my unexperience, just trying to improve the performance of the model is helping me to understand how it works. I will start reading more about active learning, at least until Monday.
2. **Course**: [Machine Learning Scientist with Python datacamp Career Track](https://app.datacamp.com/learn/career-tracks/machine-learning-scientist-with-python). Finished the *Linear Classifiers in Python* course. Nothing new but a good refresher of the theory and good to get some practice.

---

### **Days 109 of #100DaysOfData Challenge**

Today was more a discussion day at *Omdena*, not much coding but lots of time on meetings. On other terms, I finished the chapter one of the book and progressed a bit on *Coursera* and *Datacamp*.

1. **Project**: [Using Computer Vision to Detect Ethnicity in News and Videos and Improve Ethnicity Awareness](https://omdena.com/projects/ethnicity-awareness/). On one side, I am struggling to make the CNN work properly and give good accuracies. I am suffering mainly of overfitting but I can't find a good solution. So far I have increased the dropout in the hidden layers, added regularization in all of them and increased the amount of data. Also we had our brainstorming session for task 1 and task 2, quite useful to structure the work and define what needs to be done. And also, an extra meeting for task 6 to discuss how to progress.
2. **Book**: [Hands-on Machine Learning with Scikit-Learn, Keras & TensorFlow](https://github.com/ageron/handson-ml2). Just finished the first chapter of the book where it explains the basics of Machine Learning, types of algorithm, main challenges and an introduction to testing and validating. The next chapter is an end-to-end project explained.

---

### **Days 108 of #100DaysOfData Challenge**

Main achievement for today is to adapt the CNN for race classification on our *Omdena* project. But also I have progressed a bit on *Coursera* course and also took the basic ML assessment in *Datacamp*

1. **Project**: [Using Computer Vision to Detect Ethnicity in News and Videos and Improve Ethnicity Awareness](https://omdena.com/projects/ethnicity-awareness/). Just finished to adapt the CNN for race classification that I found in an [article](https://towardsdatascience.com/building-a-multi-output-convolutional-neural-network-with-keras-ed24c7bc1178), well explained and very useful. Also started to train it and seems to overfit a lot, tomorrow I will try to solve this issue to get at least normal values. Also I will be having a meeting to define better the things to be done in the short term.
2. **Course**: [Machine Learning Scientist with Python datacamp Career Track](https://app.datacamp.com/learn/career-tracks/machine-learning-scientist-with-python). Completed the basic machine learning assessment to skip some of the initial courses. The result was 177 points and over 99% of the people that took the assessment, therefore I am quite happy with that.

---

### **Days 107 of #100DaysOfData Challenge**

Today I started 3 new items, a book, a new coursera specialization and a new DataCamp career track and I plan to complete them during this month. A side of that I finished the task for face detection.

1. **Project**: [Using Computer Vision to Detect Ethnicity in News and Videos and Improve Ethnicity Awareness](https://omdena.com/projects/ethnicity-awareness/). Just completed the face detection benchmarking with *RetinaFace* and from today I will focus in the active learning task. I just found an already made CNN for race classification that I will use as starting point. Tomorrow I want to finish the first example/simulation of active learning.
2. **Course**: [Deep Learning Specialization](https://www.coursera.org/specializations/deep-learning?=&page=1). Just started the famous Deep Learning Specialization in Coursera from *DeepLearning.AI*. I hope to get well introduced to deep learning with it and be confortable using libraries like *Tensorflow* or *Keras*. So far, I just finished the introductory week 1.
3. **Course**: [Machine Learning Scientist with Python datacamp Career Track](https://app.datacamp.com/learn/career-tracks/machine-learning-scientist-with-python). Also started the Machine Learning career track in datacamp, although I had already some courses completed I will still need to do lots. I started with the tree based models course.
4. **Book**: [Hands-on Machine Learning with Scikit-Learn, Keras & TensorFlow](https://github.com/ageron/handson-ml2). Following the same line of the courses I am starting a book that is quite focused in Neural Networks with some traditional machine learning. I hope that with this combination I can master the most important libraries. So far just started with the introduction chapter.

---

### **Days 106 of #100DaysOfData Challenge**

Today I went back to face detection task in *Omdena* project and spent most of the time there. From tomorrow I will start new courses and book, so my activity will be more spread.

1. **Project**: [Using Computer Vision to Detect Ethnicity in News and Videos and Improve Ethnicity Awareness](https://omdena.com/projects/ethnicity-awareness/). Coming back to face detection, I transferred what I did in *Google Colab* to our *DAGsHub* repository and tried to code everything in a *.py* file. The only troubles I am finding is to convert back the images to video, I am having some misalignment in the setup. Once I can solve them, the task would be completed quite successfully.

---

### **Days 105 of #100DaysOfData Challenge**

Not much work done today, just continued with active learning algorithm implementation and also had some meetings, all related with *Omdena* project.

1. **Project**: [Using Computer Vision to Detect Ethnicity in News and Videos and Improve Ethnicity Awareness](https://omdena.com/projects/ethnicity-awareness/). We had our weekly meeting, the things are slowing down now. The people is now in the phase of trying to put order in the ideas and create meaningful pipeline. Also I continued trying to implement active learning algorithm, although I am starting to see some limitations. The most important the trade-off between the confidence of the autolabeled pictures and the amount of manually labeled required. Having an initial dataset of more than 100k samples will help a lot on this.

---

### **Days 104 of #100DaysOfData Challenge**

More discussions day, we had some tasks meeting on our *Omdena* project and also I started to code the active learning algorithm.

1. **Project**: [Using Computer Vision to Detect Ethnicity in News and Videos and Improve Ethnicity Awareness](https://omdena.com/projects/ethnicity-awareness/). A part from the meetings to discuss and, in this case, clarify what is the progress and how to continue, I started coding the active learning algorithm. So far, just the most simple coming from an article but the plan is increasingly improve it.

---

### **Days 103 of #100DaysOfData Challenge**

Shared the time between my own project and *Omdena*'s one, and soon I will be starting other courses and books. The planning phase has re-started.

1. **Project**: [Using Computer Vision to Detect Ethnicity in News and Videos and Improve Ethnicity Awareness](https://omdena.com/projects/ethnicity-awareness/). Not very successful day, just tried to get my face detection benchmark done but I couldn't manage because some issues with *DAGsHub*. Today I will focus more in my task and will start building the active learning algorithm.
2. **Project**: [Zoopla London Houses Price Prediction from Scratch](https://github.com/diequies/zoopla_houses). Finally managed to fill the gaps I had with in Westminster and the City of London. I realized that I need to make more robust code to be able to come back and not spend hourse just re-doing the same again. From now, I will continue with the EDA.

---

### **Days 102 of #100DaysOfData Challenge**

Today, again I spent most of the Data Science time with *Omdena*'s project while my own house pricing project is running some web scrapping on the background.

1. **Project**: [Using Computer Vision to Detect Ethnicity in News and Videos and Improve Ethnicity Awareness](https://omdena.com/projects/ethnicity-awareness/). Today's main work has been to extract and learn how to upload the *FairFace* database to *DAGsHub* and the conduct EDA to understand how useful for our purposes this dataset is. So far it looks promising, quite balanced in race feature and also if we add the gender to the equation. With regards of the age, it basically follows the normal population distribution. The plan for tomorrow is get ready my benchmark face detection algorithm, *RetinaFace*.

---

### **Days 101 of #100DaysOfData Challenge**

The most useful part of today was the tutorial and some hands-on trial with *DAGsHub*, it is the official repository for *Omdena* project and also I find it very useful for my next projects. It is basically the same as *GitHub* but adding some features specifically for Data Science, like *DVC* storage management or *MLFlow*.

1. **Project**: [Using Computer Vision to Detect Ethnicity in News and Videos and Improve Ethnicity Awareness](https://omdena.com/projects/ethnicity-awareness/). Today, a part from the *DAGsHub* activity, I came back to data labeling task. We could find just one public database that allows for commercial use, so now I am downloading it and doing some exploratory analysis to see how usable it is.

---

### **Days 100 of #100DaysOfData Challenge**

Finally day 100, more than 3 months doing at least a minimum of Data Science to reach my objective. It has been a very good way to keep the engagement. During this days I've finished an *Omdena* project and started a new one, also I have my personal project on-going. In terms of courses, I've done the Python for Data Science at Udemy, Machine learning at Coursera and 2 skill tracks in DataCamp. And finally, I read the Python Data Science Handbook and the Machine Learning Yearning books. I will continue with challenge even though it is completed, let's see if I can reach 200 days.

1. **Project**: [Using Computer Vision to Detect Ethnicity in News and Videos and Improve Ethnicity Awareness](https://omdena.com/projects/ethnicity-awareness/). I have been researching about the algorithm I am implementing for face detection, the *RetinaFace*, the code is already there in a quite simple way. So far, I have made it work, from our benchmarking video it just misses some frames where a face is occluded. The main problem I am finding is that it takes 1h to process 300 frames.
2. **Course**: [Machine Learning by Andrew Ng and Stanford University](https://www.coursera.org/learn/machine-learning/home). Finished weeks 10 and 11, they were quite short. The week 10 was about strategies to apply Machine Learning to big amounts of data, like *Stochastic Gradient Descent*. The last week of the course, the 11th, was kind of how to apply Machine Learning in a project, the importance of using pipelines and tactics to improve our scores.

---

### **Days 99 of #100DaysOfData Challenge**

I had time spend in both of my projects with quite good progress. Most of the time it has been *Omdena* but also I am trying to push to finish my project.

1. **Project**: [Using Computer Vision to Detect Ethnicity in News and Videos and Improve Ethnicity Awareness](https://omdena.com/projects/ethnicity-awareness/). Today I moved to work on a different task, I don't want to get stuck in just data gathering/labeling like in the previous challenge. I am collaborating in the face detection task, I will test RetinaFace algorithm to assess its performance.
2. **Project**: [Zoopla London Houses Price Prediction from Scratch](https://github.com/diequies/zoopla_houses). I had to go back to data ingestion code, for some reason I missed one borough and I just realized about it when started plotting maps. When revisiting the code I am seeing so many errors and bad written code, I need to start a new project to focus on the code.

---

### **Days 98 of #100DaysOfData Challenge**

Not very productive day, I just could spend the minimum time at *Omdena* project. 

1. **Project**: [Using Computer Vision to Detect Ethnicity in News and Videos and Improve Ethnicity Awareness](https://omdena.com/projects/ethnicity-awareness/). The most important today was the weekly team meeting, the PO explained a bit better how the challenge will work and also we had a quite varied discussion on how to approach the problem. Still a bit chaotic but we are slowly getting organised.

---

### **Days 97 of #100DaysOfData Challenge**

Busy day at *Omdena* having a long discussion about data labeling, but still I managed to have progress also in *Coursera* course. From today to the end of the month I will be spending more time on my personal house pricing project, I want to finish it to start a new one.

1. **Project**: [Using Computer Vision to Detect Ethnicity in News and Videos and Improve Ethnicity Awareness](https://omdena.com/projects/ethnicity-awareness/). Finally we had our first task meeting, long discussion of more than an hour but with good outputs. We set a long run plan and also an starting step to progress. Hopefully the task last just a couple of weeks or at least does not take too much of my time so I can collaborate in others. From now, we will be gathering public data sets of faces labeled by ethnic group to try to collate the most useful in our initial data set.
2. **Course**: [Machine Learning by Andrew Ng and Stanford University](https://www.coursera.org/learn/machine-learning/home). Completed week 9 of the course, the second sub-module was about recommender systems, but I was already experienced on them because of my first *Omdena* project. Anyway, it was good to review the theory again and get a better understanding. Recommender systems are a quite special problem to solve in Data Science, the approach needs be different and we learnt that the hard way.

---

### **Days 96 of #100DaysOfData Challenge**

Good progress today, on one side, the labeling task is getting a bit clearer now and then, I finished the Machine Learning Fundamentals Datacamp skill track. I still need to improve the effectiveness of my work, but now it is getting better.

1. **Project**: [Using Computer Vision to Detect Ethnicity in News and Videos and Improve Ethnicity Awareness](https://omdena.com/projects/ethnicity-awareness/). Now it seems clear that the long run strategy is labeling via *active learning*, now we have to understand better how to implement it and also how to get the initial data set. It seems that there are many already labeled data sets free and available. Tomorrow we will have the first meeting to plan the task.
2. **Course**: [Machine Learning Fundamentals datacamp Skill Track](https://app.datacamp.com/learn/skill-tracks/machine-learning-fundamentals-with-python?version=1). Completed the last course, *Introduction to Deep Learning*, very simple but useful to understand the fundamentals applying them. 
3. **Course**: [Machine Learning by Andrew Ng and Stanford University](https://www.coursera.org/learn/machine-learning/home). Just finished the anomaly detection module, it is the first time I hear about this type of machine learning. It is interesting to understand why it is different to usual supervised learning problems. Being such a small amunt of positive cases and so different each other, we cannot pretend to learn about the anomaly and detect it, we have focus on what is out of the normal behaviour.

---

### **Days 95 of #100DaysOfData Challenge**

Intense day in *Omdena* project, a lot of discussions going on and just keep up with them takes a good amount of time. Also I am keeping the progress on my courses to meet my target, but I feel I am leaving behind the house pricing project.

1. **Project**: [Using Computer Vision to Detect Ethnicity in News and Videos and Improve Ethnicity Awareness](https://omdena.com/projects/ethnicity-awareness/). Today I created data labeling task and became the task leader, not sure if it would be useful for me and if I would struggle because I had no experience on that. So far, the most important part is to study and gather the tools and strategies to decide which one to take.
2. **Course**: [Machine Learning Fundamentals datacamp Skill Track](https://app.datacamp.com/learn/skill-tracks/machine-learning-fundamentals-with-python?version=1). The fourth course was a walktrough project of basic NLP, where we built slowly a quite long pipeline to solve the problem. I haven't use pipelines yet in real problem, but it seems very useful to simplify the code and keep it organised.
3. **Course**: [Machine Learning by Andrew Ng and Stanford University](https://www.coursera.org/learn/machine-learning/home). Completed the week 8 focused on unsupervised learning, mostly K-Means Clustering and PCA for dimensionality reduction. It is not the first time I see these algorithms, but Andrew Ng explains quite thoroughtly and better the theory and the use cases.

---

### **Days 94 of #100DaysOfData Challenge**

A lot of time spent today introducing myself in computer vision, but also I had some progress in *DataCamp* and also in the *Coursera* course.

1. **Project**: [Using Computer Vision to Detect Ethnicity in News and Videos and Improve Ethnicity Awareness](https://omdena.com/projects/ethnicity-awareness/). My main objective at the moment is to get to a minimum knowledge at computer vision field. I have been reading several introductory post and now I will be moving to more complex papers. Also I am thinking to take the lead in data labeling task, probably the most simple in this project.
2. **Course**: [Machine Learning Fundamentals datacamp Skill Track](https://app.datacamp.com/learn/skill-tracks/machine-learning-fundamentals-with-python?version=1). Just finished the third course in the track about Linear Classifiers. We reviewed the theory basics and most importantly we had hands-on examples of the most important, Logistic Regression (with its regularizations) and Support Vector Machine. After having the theory from other sources, it is useful *DataCamp*'s more practical approach.
3. **Course**: [Machine Learning by Andrew Ng and Stanford University](https://www.coursera.org/learn/machine-learning/home). Completed the week 7 about Support Vector Machines. Again, the best part of the course is the theoretical approach, much deepper than other sources. It is really useful when it is combined with a more practical approach like with DataCamp. SVMs are like the next step after the linear classifiers, using different kernels it can classify using non-linear boundaries.

---

### **Days 93 of #100DaysOfData Challenge**

Two main items to highliht today, the kick-off of *Omdena* project and finishing the book *Machine Learning Yearning*. 

1. **Project**: [Using Computer Vision to Detect Ethnicity in News and Videos and Improve Ethnicity Awareness](https://omdena.com/projects/ethnicity-awareness/). The kick-off meeting with CeretAI happened today, most of anything I am amazed to see how clear their problem statement is. Also, there are many more collaborators and more experienced. The deliverable of the project is to create an algorithm that can detect people and its ethnicity in broadcasting videos with at least a 95% of accuracy. As first day of the challenge I am very happy with what I've seen, now we have to gather as much information as we can.
2. **Book**: [Machine Learning Yearning](https://github.com/ajaymache/machine-learning-yearning). Just finished the book, it is quite short and it goes to the point quite directly. The book tries to explain how to approach machine learning projects, showing the most likely problems you may encounter. And what is more important, it speaks about strategies to tackle them. At the end it is a reference book that I am sure I will consult in the future but also gave good insights.


---

### **Days 92 of #100DaysOfData Challenge**

As the new challenge from *Omdena* will be using *DAGsHub* instead of *GitHub*, I spent most of the time learning and setting up an account there.

1. **Project**: [Using Computer Vision to Detect Ethnicity in News and Videos and Improve Ethnicity Awareness](https://omdena.com/projects/ethnicity-awareness/). Even though the project kick-off is tomorrow, I am doing some initial preparations. The most important has been learn the new repository site and methodology, *DAGsHub*. At first sight it looks very much like *GitHub* but with some extra features specifically for Data Science.
2. **Course**: [Machine Learning Fundamentals datacamp Skill Track](https://app.datacamp.com/learn/skill-tracks/machine-learning-fundamentals-with-python?version=1). Completed the second course about unsupervised learning algorithms. I learnt about some new algorithms for me, like *Non-negative Matrix Factorization*, that looks useful for NLP or image processing. And also, again, you get to use the algorithms yourself, even though it is a very controlled environment, it makes the difference.


---

### **Days 91 of #100DaysOfData Challenge**

Not a massive progress today, just a little bit on each item. The only interesting one would be on my house pricing project, where I am learning how to use *geopandas* package.

1. **Project**: [Zoopla London Houses Price Prediction from Scratch](https://github.com/diequies/zoopla_houses). I have been learning how to use *geopandas* package as I am facing some issues with just one of the boroughs that is not showing in any plot. So far, I just know that the problem comes when I merge the dataframe from *geopandas* with another dataframe from normal *pandas*. I need to keep studying the issue to find where I have the leakage of information.

---

### **Days 90 of #100DaysOfData Challenge**

The main progress today is that I finished the week 6 in *Andrew Ng*'s course, started a new book, *Machine Learning Yearning*, also from *Andrew Ng* and completed a datacamp course.

1. **Course**: [Machine Learning by Andrew Ng and Stanford University](https://www.coursera.org/learn/machine-learning/home). Completed the week 6, where mainly it was introduce  the bias-variance problem and trade off. Also, and most interesting, not only explained the problem, but also different ways to solve it depending on the situation. For example, when it is useful to increase the training data, or when to go to a simplet/more complex algorithm, etc. Even though they were concepts that I already knew, it was very useful to go deeper on them.
2. **Book**: [Machine Learning Yearning](https://github.com/ajaymache/machine-learning-yearning). Justs started the book but it looks very insightful. At first sight, the main take away is how to aproach Machine Learning Projects, points to take into account, the ones to avoid, etc.
3. **Course**: [Machine Learning Fundamentals datacamp Skill Track](https://app.datacamp.com/learn/skill-tracks/machine-learning-fundamentals-with-python?version=1). Just completed the first course of the track about supervised machine learning algorithms. I didn't learn anything about new algorithms, but it was useful to get hands-on activities with pipelines mainly. In my housing prices project I will put in practice.

---

### **Days 89 of #100DaysOfData Challenge**

Finally finished the book *Python Data Science Handbook* and also progressed with the EDA on my house pricing project. Being real data scrapped by myself, I am finding very interesting insights and I guess quite unique as I am not using public available data.

1. **Project**: [Zoopla London Houses Price Prediction from Scratch](https://github.com/diequies/zoopla_houses). Today I started to try geographical plotting for the first time. It is different and complicated at first, but with *geopandas*, it gets simple quickly. The next step would be to customize a bit the plot and make nice visualizations.
2. **Book**: [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/). Book completed, even though it is a reference book, it has been very useful for me to read it. Mainly because I get to know the possibilities of packages or models, so afterwards when I want to use them I can have ideas of what to do. Also, in the last part about *Scikit-Learn*, it introduces many algorithms, some new for me.

---

### **Days 88 of #100DaysOfData Challenge**

Not a very successful day but I managed to spend some time in each of my open fronts.

1. **Project**: [Zoopla London Houses Price Prediction from Scratch](https://github.com/diequies/zoopla_houses). Not much progress, I just started to define all the variables I obtained from the previous stage and planned a bit what could be interesting from them. 
2. **Course**: [Machine Learning by Andrew Ng and Stanford University](https://www.coursera.org/learn/machine-learning/home). I finished the introduction and intuition of *neural networks*. It is true that it does not solve any practical example but the explanation is very clear to have good foundations.

---

### **Days 87 of #100DaysOfData Challenge**

I spent most of the time in the zoopla project, I had to go back to speed and remember what I was doing. Also I had some side time to progress in the courses and book.

1. **Project**: [Zoopla London Houses Price Prediction from Scratch](https://github.com/diequies/zoopla_houses). I completed the data extraction and cleaned a little bit the data. Now I have a table in a local MySQL database with all the information in quite a good condition. My next step is to do EDA on that data, let's see what we can extract.

---

### **Days 86 of #100DaysOfData Challenge**

After two weeks of not being able to work at all, I am back to the business and the challenge full time. I have just completed Udemy course and started the famous *Machine Learning Course from Stanford University in Coursera*.

1. **Course**: [2021 Python for Machine Learning & Data Science](https://www.udemy.com/course/python-for-machine-learning-data-science-masterclass/). I took the last modules on this course, *DBSCAN* and *PCA*. These algorithms are not so obvious to find a use of it, but I find them very useful for data exploration and visualization. Now that I finished the course I can say that it is really useful for beginners like me. It provides a good foundation of Machine Learning, with some theory on each algorithm (of the most used ones) and an applied example of each as well.
2. **Course**: [Machine Learning by Andrew Ng and Stanford University](https://www.coursera.org/learn/machine-learning/home). I started and completed the first weeks of the course. The main reason to do the course is to get a bit deeper on the theory of the main algorithms before moving to the next steps. On the first weeks of the course it focuses in *Cost Functions*, *Gradient Descent Theory*, *Various types of Regressions* and started to introduce *Neural Networks*.

---

### **Days 85 of #100DaysOfData Challenge**

With the moving I am not having too much time for data science these days. Basically, I am reading the book and watching Udemy course, hopefully soon I get more time to progress in my project.

1. **Course**: [2021 Python for Machine Learning & Data Science](https://www.udemy.com/course/python-for-machine-learning-data-science-masterclass/). Today I have finished the module of *Agglomerative clustering* algorithm. It follows similar logic than yesterday's module, it will group the samples in clusters based on spatial distances. I have the same feeling, it seems more an explorative tool than something to give final results.
2. **Book**: [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/). This chaper is running trhough the main types of algorithms, showing a short description, the method, when to use it and a final example. It is quite good to get an overview, but still it was not the main purpose of the book and the course is giving similar but deeper information.

### **Days 84 of #100DaysOfData Challenge**

Another courses day, I want to clean a bit and close the courses I had opened before moving on. Also I found an interesting post about data projects structure that I want to apply to the London Housing Price project.

1. **Course**: [2021 Python for Machine Learning & Data Science](https://www.udemy.com/course/python-for-machine-learning-data-science-masterclass/). Today I have finished the module of *K Means Cluster* algorithm. Unsupervised learning is a bit less straightforwards, mainly to understand how to evaluate the performance. The course shows some simple examples of how to use this clustering algorithm, such as color reduction in an image or to group similar countries, etc. It seems like this algorithm is the mid point of a more deeper study, whereas the supervised learning algorithms are more the ouput of a project.
2. **Book**: [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/). I have started the last module of the book, about machine learning in general, but focused mainly in *Scikit-Learn*. So far, I have read the introduction to the library, it explains how is structured and how it works. It is interesting to see as overall, how all the algorithms or processes are very similar, so it is simple to do very useful pipelines. Definitely I need to practice pipelines on my personal project.

---

### **Days 82-83 of #100DaysOfData Challenge**

Now that *Omdena* project is almost finish and there is very little work to do, I am focusing in complete the open courses and book a I had. Also, I will be getting back my London housing project very soon.

1. **Course**: [2021 Python for Machine Learning & Data Science](https://www.udemy.com/course/python-for-machine-learning-data-science-masterclass/). Just finished the *Naive Bayes Classification applied to NLP* module. The algorithm itself is really simple and does not have even hyperparameters to tune, but the *NLP* part is very interesting. Even though it is just an introduction, it has attracted my attention.
2. **Course**: [Python Programming datacamp Skill Track](https://learn.datacamp.com/skill-tracks/python-programming?version=2). I have also completed the *Programming Skill Track* at *Datacamp*, I am still not sure how common is *OOP* in data science, but it seems very important to learn towards getting a job. I will try to apply what I have learnt in the London Housing Price Project.
3. **Book**: [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/). Also, I finished the *MatPlotLib* module. I already had experience with this library and also with *Seaborn*, but it has been useful to settle the *OOP* way to use it and get a glimpse of the very extense options to personalize our visualizations. The next module is about machine learning, mainly with *Scikit-Learn*, again I have already experience but I am sure it would be useful.

---

### **Days 80-81 of #100DaysOfData Challenge**

Quite productive weekend, even though I couldn't spend too many hours on data science. From one side I completed the demo for *Omdena* and then also completed another module of the Udemy course. Now that I will have more time, I hope I can speed up all the activities.

1. **Project**: [Building A Recommendation System For Cognitive Exercises and Training Programs](https://omdena.com/projects/cognitive-exercises/). Finally I have completed the demo task, it will be a presentation with some results to show the performance of the model and also a couple of GIF to show the results. From now, the last activity of this project would be to get ready for the final presentation.
2. **Course**: [2021 Python for Machine Learning & Data Science](https://www.udemy.com/course/python-for-machine-learning-data-science-masterclass/). The module I have just finalised is the boosting algorithms module. The course touches upon *Adaboost* and *Gradient Boosting* algorithms, both following similar logics. Boosting algorithms apply a specific algorithm (in this case decision trees) repeteadly in series, each one learning from the previous one. Boosting methodology can be very powerful as it combines the power of multiple algorithms.

---

### **Day 79 of #100DaysOfData Challenge**

Day focused on finishing the demo for *Omdena* project, also I try to use some time to read the book, but the progress is very slow.

1. **Project**: [Building A Recommendation System For Cognitive Exercises and Training Programs](https://omdena.com/projects/cognitive-exercises/). Today I got the final draft for the demo, probably the only work remaining is to format the document. The demo shows some sense for the model, although it shows also some deficiencies. But there is no more time in the project to fix and improve the models. Anyway, it is a great success to deploy a recommendation system at almost production level in such a short term.

---

### **Day 78 of #100DaysOfData Challenge**

Last days to complete the demo and presentation of *Omdena* project. We are running late, so these days I am completely focused on finishing something I can show as a demo.

1. **Project**: [Building A Recommendation System For Cognitive Exercises and Training Programs](https://omdena.com/projects/cognitive-exercises/). I am struggling to get the appropriate results to make a story as a demo. Currently I am getting very similar results without data and with user data, we need to confirm the logic behind.

---

### **Day 75-77 of #100DaysOfData Challenge**

The days 75 to 77 have been quite out of data science and coding. It has been my last week at work and most of the time I had to spend to close everything. Anyway, I tried to always spend a minimum time of coding to keep the progress, mainly with Maria's code and *Omdena*'s project.

1. **Project**: [Building A Recommendation System For Cognitive Exercises and Training Programs](https://omdena.com/projects/cognitive-exercises/). I've been focused on the demo, but as the other code was not completed yet I was getting faulty results. It seems that now the latest code has been merged, therefore tomorrow I will test the models and try to make a demo.

---

### **Day 74 of #100DaysOfData Challenge**

Another day working to have a working demo and also some statistics for the different scenarios. Also, as everyday I had some time with Maria's code, now I am close to finish the translation from R to Python.

1. **Project**: [Building A Recommendation System For Cognitive Exercises and Training Programs](https://omdena.com/projects/cognitive-exercises/). I started to review the whole app code, it is production code based on OOP. So far, it is being complicated to understand it. Although I managed to get the code to do some recommendations I still cannot fully understand it. I believe it is very important to me to understand the code, its logic and be able to reproduce it. It is the next level of coding.

---

### **Day 73 of #100DaysOfData Challenge**

Today I had time to spend in the Omdena project, not only coding but more important, having a meeting to define the steps. As always, also spent some time with Maria's code, which is actually being quite educational as Pandas and Numpy practice.

1. **Project**: [Building A Recommendation System For Cognitive Exercises and Training Programs](https://omdena.com/projects/cognitive-exercises/). From the meeting we defined the scenarios we want to show in the demo, and I have almost finished coding the database variations for them. The next step will be to understand the overall code to be able to call the API and get recommendations, which will not be trivial for me.

---

### **Days 70, 71 and 72 of #100DaysOfData Challenge**

The first two days I have been so busy that I could just spend some time in Maria's code. They day 72 I could have more time and progress in almost all the open fronts. Hopefully I can avoid in the future such improductive days in a row.

1. **Project**: [Building A Recommendation System For Cognitive Exercises and Training Programs](https://omdena.com/projects/cognitive-exercises/). I have finished the script to generate the demo database for different scenarios. But now I am struggling to come up with a metric to meassure the success of our recommendation. Tomorrow we will have a meeting to discuss this point and the possible scenarios.
2. **Project**: [Zoopla London Houses Price Prediction from Scratch](https://github.com/diequies/zoopla_houses). Finally, I have completed the data extraction for this project. It has been long, and also along the process I have understood errors I will not make again. For my next personal project I will look for something more dynamic, not like this that takes 2 weeks to extract the data. The next step in this project is the EDA.
3. **Course**: [2021 Python for Machine Learning & Data Science](https://www.udemy.com/course/python-for-machine-learning-data-science-masterclass/). I have finalised the Random Forest Algorithm module. It is very interesting how it is combined the strength of decision trees with other statistics methods to face its weaknesess. Random forest logic is to use repeatedly decision trees with a sample of the data, records and/or features. And then, combine them to find the best metrics, not only for the data we use to train but also to generalise to validation data or unseen data.
4. **Course**: [Python Programming datacamp Skill Track](https://learn.datacamp.com/skill-tracks/python-programming?version=2). Today I completed the software engineering for data science course. It is a very general overview, but it has given me a much better understanding of classes, functions, documentations, unit tests, etc. The next course will be about unit testing.

---

### **Day 69 of #100DaysOfData Challenge**

Not a productive day at all. I wanted to progress and almost complete my demo proposal, but I found a small error that took the day to solve. And of course, I spent some time with Maria's code.

1. **Project**: [Building A Recommendation System For Cognitive Exercises and Training Programs](https://omdena.com/projects/cognitive-exercises/). While generating the data for the demo, I found that the users were not being generated properly. I shuffled the data to avoid it to be completely sorted, but I did it after extract some of the features. Therefore I have been working with users in different places with different IDs. Now I sorted it out and the flow is correct.

---

### **Day 68 of #100DaysOfData Challenge**

Today I have mainly continued with the script to generate different databases for demo **Omdena** project. Also, I kept progressing Maria's code, now I am closer to complete it.

1. **Project**: [Building A Recommendation System For Cognitive Exercises and Training Programs](https://omdena.com/projects/cognitive-exercises/). Even though the limitations I have with sqlalchemy loading files and queries, I manage to finialize the script to generate the databases. I am not sure why but I can't execute more than one consecutive query, if I do, the second one fails. Now I have to think in the logic and the metric to measure recommendation success. 

---

### **Day 67 of #100DaysOfData Challenge**

Today I was concentrated on generating the demo databases in **Omdena** project. I spend also some time with Maria's code to keep progressing.

1. **Project**: [Building A Recommendation System For Cognitive Exercises and Training Programs](https://omdena.com/projects/cognitive-exercises/). My main objective now in **Omdena** project is to get a working demo, and to achieve that objective my task would be to generate the different required databases. My first struggling point is to execute sql scripts from a Notebook, because of some issues with MySQL I couldn't execute more than one. Finally I copied all the scripts in one sql script and it worked, but I don't think it is the proper way. I should understand why it was failing.

---

### **Day 66 of #100DaysOfData Challenge**

As today I had many personal appointments I just could spend a little of time with coding. I used the time I had to finalize decision trees algorithm module in *Udemy* course.

1. **Course**: [2021 Python for Machine Learning & Data Science](https://www.udemy.com/course/python-for-machine-learning-data-science-masterclass/). Decision tree algorithms are one of the most "understandable" or better to visualize to people out of Data Science. They are based on decisions to separate or classify the data and these can be quite easy to see. One of the main problem of them is overfitting, to avoid overfitting we have to apply techniques such as prunning. Decision tree algorithms are the initiation of some of the most powerful algorithms like random forest or boosting algorithms. Between other improvements, they handle much better the problem of overfitting. The next module is about random forest algorithms.

---

### **Day 65 of #100DaysOfData Challenge**

Unfortunately, today I had an issue with Visual Studio and one notebook that seemed to be corrupted. Basically, I spent most of the time trying to solve the problem, finally I had to copy the code itself in a new notebook. The small amount of remaining time I could spend it in **Omdena** project.

1. **Project**: [Building A Recommendation System For Cognitive Exercises and Training Programs](https://omdena.com/projects/cognitive-exercises/). During the code walktrough I realized that the meaningful data I was generating was biased. When I assigned the joining date to each user, I was doing it in order but as the users were already sorted by state, the result was that the user were joining in groups per state.

---

### **Day 64 of #100DaysOfData Challenge**

Not a very productive day in regards of coding, probably the most useful has been to complete a Datacamp course about decorators.

1. **Project**: [Building A Recommendation System For Cognitive Exercises and Training Programs](https://omdena.com/projects/cognitive-exercises/). Today we had a code walkthrough with the partners, I didn't completely understand what was the meaning of it. My presentation was too detailed, but once I got the feedback, everything improved. From now I know what a code walkthrough is.
2. **Course**: [Python Programming datacamp Skill Track](https://learn.datacamp.com/skill-tracks/python-programming?version=2). Today I completed the course to write functions in python, the first part was quite straightforwards but then, once they introduced the decorators it got much more interesting. The decorators are functions that modify other functions, I know now how it works but I need to find a use and apply it to settle the knowledge.

---

### **Day 63 of #100DaysOfData Challenge**

Today has been a bit of mixed day, I didn't have too much time and I spent it half in Maria's code and the other half divided between **Omdena** project and both courses I am doing.

1. **Project**: [Building A Recommendation System For Cognitive Exercises and Training Programs](https://omdena.com/projects/cognitive-exercises/). Mainly I have been tracking current discussions and tried to read about how to divide the database in training/test/validate sets. In recommedation systems, the way we do it is completely different. Tomorrow I will continue reasearching.
2. **Course**: [Python Programming datacamp Skill Track](https://learn.datacamp.com/skill-tracks/python-programming?version=2). Today I started the course of *Writing functions with Python*. I already knew how to write a very basic function, but it includes details of how to document, what variables are and how they behave in the context, etc. Tomorrow I will do the decorators part, in which I have quite a lot of interest.
3. **Course**: [2021 Python for Machine Learning & Data Science](https://www.udemy.com/course/python-for-machine-learning-data-science-masterclass/). Also started the *SVM* module of *Udemy* course. We have covered the theory part, where they explain from the basic linear support vector to the use of Kernels to separate the different classes. Having different kernels in the algorithm and each kernel different parameters, it is a model where the hyperparameter tunning with cross validation is very important, as well as to normalize the features as the algorithm uses the space distances.

---

### **Day 62 of #100DaysOfData Challenge**

Today I couldn't spend too much time on coding and data science, mainly I corrected some problems in **Omdena** database and also spent some time translating code from *R* to *Python* for Maria's Phd.

1. **Project**: [Building A Recommendation System For Cognitive Exercises and Training Programs](https://omdena.com/projects/cognitive-exercises/). Just some minor corrections in variable types and I guess some more corrections coming in the future.
2. **Project**: [Zoopla London Houses Price Prediction from Scratch](https://github.com/diequies/zoopla_houses). Pub and restaurands around the house in 1km diameter feature is completed, has taken probably too long through the API but I cannot think in a better way. Now I started with education buildings, and I guess it will get the same amount of time. I am seeing that I will spend the month just waiting for the API to download everything I need...

---

### **Day 61 of #100DaysOfData Challenge**

Today was a productive day, I had a lot of time to spend and I could progress with the new course, the book and also thinking in the new task for **Omdena**. We are almost finished, so everyone is trying to finalize their tasks.

1. **Project**: [Building A Recommendation System For Cognitive Exercises and Training Programs](https://omdena.com/projects/cognitive-exercises/). Today I just explained a couple of ideas of how to demo our models. I did not have a huge interest from the other people, it is actually a lower priority, but I will prepare some databases to be able to explain the idea better.
2. **Course**: [Python Programming datacamp Skill Track](https://learn.datacamp.com/skill-tracks/python-programming?version=2). Today I completed the course I started yesterday, *Writing efficient python code*, and also the next one, *Writing efficient code with Pandas*. They were not exactly what I was looking for, but they have been very helpful. Now I understand how to use quite common Python functions to have faster and more efficient code, also with *pandas*. I need to put in practice what I learnt, otherwise I will forget it.
3. **Book**: [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/). The part I read today was all about Date and Time functions and data types. It explains how base *Python* handles it with some limitations on efficiency, *NumPy* also do but with some limitations in functionalities and finally *Pandas* combines the best of both sides. I need to make a time series project to put in practice all these functions.
4. **Course**: [2021 Python for Machine Learning & Data Science](https://www.udemy.com/course/python-for-machine-learning-data-science-masterclass/). Today I finished the *K Nearest Neighbors (KNN)* algorithm module. It is interesting because it is very simple and easy to understand, it would be perfect to explain results to people out of the field. Although it has many limitations in terms of performance, mainly if we want to apply to regression or in classification problems with high dimensionality. The next module is *Support Vector Machines (SVM)*, which is quite interesting but also more complicated and intense in maths.

---

### **Day 60 of #100DaysOfData Challenge**

Today I started a skill track in datacamp, *Python Programming*, I definitely want to improve my coding skills a part from Data Science skills. Also could manage to spend some time in my **Omdena** project and with the Python book.

1. **Project**: [Building A Recommendation System For Cognitive Exercises and Training Programs](https://omdena.com/projects/cognitive-exercises/). Not a huge progress, added the last scripts to the documentation and re-organized a bit the folder to be more clear. My next step would be to support the demo task, I will propose some ideas.
2. **Course**: [Python Programming datacamp Skill Track](https://learn.datacamp.com/skill-tracks/python-programming?version=2). Completed the first two modules of the initial course (Writing efficient python code), it went through the basic of the efficiency in Python and how to measure the time and space used by our code using *line_profiler* and *memory_profiler*.
3. **Book**: [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/). Today I read mainly about string functions in *Pandas*. How to vectorize string actions and what functions are available, including some very useful *regular expressions* that I used in my personal project.

---

### **Day 58-59 of #100DaysOfData Challenge**

I aggregate days 58 and 59 because, even though I could spend time in data science, the progress was no so high. I could increase the items I am focusing on, now I have active both projects and the book again.

1. **Project**: [Building A Recommendation System For Cognitive Exercises and Training Programs](https://omdena.com/projects/cognitive-exercises/). During these days I kept re-organizing, tidying up the code and completing the documentation. We are on our last week and it seems we would have to do a walkthrough of our code.
2. **Project**: [Zoopla London Houses Price Prediction from Scratch](https://github.com/diequies/zoopla_houses). Finally I found an alternative API to request pub and restaurants information, it is called **OSMNX** and still uses Open Street Maps. But in this case handles better the timeouts in hardly fails. It is still quite slow, probably a couple of days to gather the info of 18k houses.
3. **Book**: [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/). It has been I while since the last time I read the book. These days I progressed in the *Pandas* chapter, specifically using *groupby* and *pivot_table*. They are quite basic functions of *Pandas*, but I could learn a some further functionalities quite interesting. Also, how they work internally. Understanding their internal pipeline, makes easier to understand what actually happens when you call them. In summary, it splits the data by the varieble we specify, then apply the function (sum, mean, min, etc.) and finally combine the data again.

---

### **Day 57 of #100DaysOfData Challenge**

Another day focused in documentation, probably the last one, but then I will need to find another task in **Omdena**. The most likely is that I will try to understand the rest of the code, although it is very high level.

1. **Project**: [Building A Recommendation System For Cognitive Exercises and Training Programs](https://omdena.com/projects/cognitive-exercises/). Nothing special today, just kept writing the documentation of my Task. Now it is almost finished, I don't think it will reveal more problems in the code. I need to find the next steps.
---

### **Day 56 of #100DaysOfData Challenge**

Less time to code today, but I spent the whole time at **Omdena** project solving some structural problems. Also I started to look to the main code, but so far, it looks too complicated. I definitely need to learn coding skills.

1. **Project**: [Building A Recommendation System For Cognitive Exercises and Training Programs](https://omdena.com/projects/cognitive-exercises/). While doing the documentation, specifically the pipeline to generate the data, I realized that it was too complicated, too many code files and not with the correct flow. So I spent the day re-structuring the code and making a smooth pipeline, now it is much easier to explain and therefore easier to go through. Tomorrow I will try to complete the documentation.

---

### **Day 55 of #100DaysOfData Challenge**

Today I managed to not only progress in **Omdena** project, but also to take back again my personal project on London house prices prediction. It is being difficult to find the time, but I should manage it better.

1. **Project**: [Building A Recommendation System For Cognitive Exercises and Training Programs](https://omdena.com/projects/cognitive-exercises/). Main focus for todays has been to solve a bug I had in my task code and then keep going with the documentation. As I see it, I believe that it should be finished by today or tomorrow.
2. **Project**: [Zoopla London Houses Price Prediction from Scratch](https://github.com/diequies/zoopla_houses). Definitely I need to start documenting my project. Every time I take it back, I need at least 30 mins to remind myself how it was working. My main issue now is the *Overpass Turbo API*, it is not meant to be used for too many requests and gives me error too frequently. I have to find an alternative.

---

### **Day 54 of #100DaysOfData Challenge**

The main point today was to adapt some feedback I had for my **Omdena** task and then start creating documentation. I learn today that the notebooks are very useful to show your work but from coding point of view, it is difficult to keep the flow and when someone else wants to run the same code, either they follow the same order or it wouldn't work. I should move to script and OOP.

1. **Project**: [Building A Recommendation System For Cognitive Exercises and Training Programs](https://omdena.com/projects/cognitive-exercises/). Main task today, documentation, it is boring but it is useful to review the code, detect problems and also bad quality in the code. I should work in improve the quality of my code, probably reviewing other **Omdena** people's code would help me with that.

---

### **Day 53 of #100DaysOfData Challenge**

The focus of today was **Omdena** project, but I managed to get some time also to see some videos of the python course. I need to start to distribute the time again between all four items.

1. **Project**: [Building A Recommendation System For Cognitive Exercises and Training Programs](https://omdena.com/projects/cognitive-exercises/). Today I finalized the last deliverable of my task, meaningful synthetic data. From now I have to complete the documentation and then I will try to move on to support other tasks and try to learn. This is the last sprint.
2. **Course**: [2021 Python for Machine Learning & Data Science](https://www.udemy.com/course/python-for-machine-learning-data-science-masterclass/). Today I have also completed the Logistic Regression module, it covered not only the normal Logistic Regression, but also multi-class classification with Logistic Regression. The logic of consecutive algorithms is really interesting.

---

### **Day 52 of #100DaysOfData Challenge**

Today was a mostly relaxing day, although I still spent some hours in **Omdena** project to progress it. 

1. **Project**: [Building A Recommendation System For Cognitive Exercises and Training Programs](https://omdena.com/projects/cognitive-exercises/). Today I almost finished the code to generate meaningful data and started the one to assign names for demo exercises. But in the meeting with the client, some assumptions changed and I will have to address them now.

---

### **Day 51 of #100DaysOfData Challenge**

Today we had a sort of alignment sprint, being my first project in this industry it is very insightful for me to see how different are the processes and management. Again, and probably until my task finishes, I will keep focus in **Omdena**.

1. **Project**: [Building A Recommendation System For Cognitive Exercises and Training Programs](https://omdena.com/projects/cognitive-exercises/). Today I kept progress in my code to generate meaningful data. The code is nothing special but it uses a big variety of functions. I have to generate controlled random values with loads of constraints. My plan is to finish tomorrow or at least before the end of the weekend. 

---

### **Day 50 of #100DaysOfData Challenge**

Midway of the challenge, I believe I was expecting more progress but with 14 holidays in between and working full time in another job, it has been really difficult. Today I could spend most of the day coding for our **Omdena** project, which is important to learn.

1. **Project**: [Building A Recommendation System For Cognitive Exercises and Training Programs](https://omdena.com/projects/cognitive-exercises/). Yesterday I finalize my clustering code, I believe it is not very orthodox, but I fulfills the objective I had. As I wanted a specific number of clusters, I had to diverge from what was the most accurate. But now I am moving to create historical data with those clusters, I have good expectatives about this.

---

### **Day 49 of #100DaysOfData Challenge**

I focused today on the clustering problem to generate meaningful data for our **Omdena** project. I couldn't spend time in other parts of my objectives, but I could learn limitation and difficulties of cluster algorithms.

1. **Project**: [Building A Recommendation System For Cognitive Exercises and Training Programs](https://omdena.com/projects/cognitive-exercises/). My plan was to find a number of clusters and be able to represent them using t-SNE, but I end up doing something completely different. First t-SNE does not give you components, therefore it is not possible to explain or understand the logic behind. Also, when I tried to do clusters using demographic and geographic data, I found that they were, at least initially, incompatible. If I used geographical data, the cluster had worse defined demographic characteristics and a meaningless spatial distribution. To have a quick solution, I have conducted an initial clustering with demographics and a second one with geographic data. It is not ideal but it worked quickly, if I had I would have study the spatial distances and tried to optimize the algorithm to use. 

---

### **Day 48 of #100DaysOfData Challenge**

Today I just worked on **Omdena** project, we had some feedback about the database and also I wanted to progress with the clustering code. Now it really feels like we are in the last weeks of the project and we have some pressure to finish at least a first prototype

1. **Project**: [Building A Recommendation System For Cognitive Exercises and Training Programs](https://omdena.com/projects/cognitive-exercises/). Today, the most interesting I did was start with the clustering code. I plan to test different algorithms and also I had the challenge to find a good way to visualize the clusters. Today I fixed this last point, using *t-distributed Stochastic Neighbor Embedding* (*TSNE*) I can reduce the dimensions to fit in a 2D or 3D plot. I tried also *PCA* and *NMF* but the  result is not as good, the clusters were not clearly visible. Also I tried K-Means algorithm for clustering, but before digging a bit more, the results do not look very reliable. Tomorrow I hope I can try other algorithms.

---

### **Day 47 of #100DaysOfData Challenge**

Today I just worked on **Omdena** project, I had not much time to spend in Data Science. But, it has been a useful time. We have had two meetings to organize and then I started a new important sub-task.

1. **Project**: [Building A Recommendation System For Cognitive Exercises and Training Programs](https://omdena.com/projects/cognitive-exercises/). As I said, today I moved to a new part of the data generation. Now I am trying to generate meaningful data, to do so,  have started by finding clusters in users data. I plan to apply some different cluster algorithms and compare the results and then try to visualize the results using PCA (Principal Component Analysis).

---

### **Day 46 of #100DaysOfData Challenge**

Finally I had the chance to re-start my side project of London houses pricing, not without difficulties... Also I keep the progress with **Omdena** project at the end of the 5th week.

1. **Project**: [Building A Recommendation System For Cognitive Exercises and Training Programs](https://omdena.com/projects/cognitive-exercises/). Now I have started to document the process we have followed to generate the data. It is the first time I have to create software documentation, new thing to learn. So far *Markdown* code is being really useful.
2. **Project**: [Zoopla London Houses Price Prediction from Scratch](https://github.com/diequies/zoopla_houses). The first day coming back to this project after some weeks. First point I have realized is that my comments were not enough, I needed too much time reviewing the code to get up to speed. If I were someone out of the code, it would be impossible for me to work on that code. Another issue I had today is involving *GitHub*. Before holidays I commited I huge amount of updates, including a *CSV* file of more than 100MB, which is not permitted by *GitHub*. I found myself in a roundabout where I couldn't push my changes but also I couldn't un-commit the change. I had to start again almost a new repository to solve this issue, but the lesson is, commit small changes and with care.

---

### **Day 45 of #100DaysOfData Challenge**

Another productive day working mainly in SQL for **Omdena** project. Sadly I have not been able to progress in any of my side projects, I need to make an effort to distribute the time.

1. **Project**: [Building A Recommendation System For Cognitive Exercises and Training Programs](https://omdena.com/projects/cognitive-exercises/). Today we have finalized the database setup, created the VIEWs, which was something new for me. After too many time spent in management of the task, I need to engage with coding, otherwise I will never learn.

---

### **Day 44 of #100DaysOfData Challenge**

Today has been a very productive day, I coded a lot, not only in Python but also in SQL, and mainly for the **Omdena** project. Hopefully as part of the ramping up, I re-start also my personal project of London house pricing.

1. **Project**: [Building A Recommendation System For Cognitive Exercises and Training Programs](https://omdena.com/projects/cognitive-exercises/). Today we have completed the sub-task related with MySQL database. I have almost zero knowledge on SQL and it has helped me to learn. I will try to finish MySQL by today. We still have to create MySQL views, which is something I have no idea.

---

### **Day 43 of #100DaysOfData Challenge**

Lately I am struggling to spend time in other things than the **Omdena** project, I guess it is because we are in the highest moment of the sub-task, but I need to find time to progress on the other items.

1. **Project**: [Building A Recommendation System For Cognitive Exercises and Training Programs](https://omdena.com/projects/cognitive-exercises/). Today I have been correcting some code and I had some issues with an apply function. I need to review why an apply function gives the error of too many values, in my understanding it just gets one value.

---

### **Day 42 of #100DaysOfData Challenge**

Another busy day. I have been focused completely on **Omdena** project, mainly organizing and refining the mid-term presentation.

1. **Project**: [Building A Recommendation System For Cognitive Exercises and Training Programs](https://omdena.com/projects/cognitive-exercises/). Even though I have been focused in non-coding stuff, I could start the correction of part of my code. Some assumptions changed and they affect to this code, now I understand how useful is to code in a modular way with functions. I need to learn to code with OOP.

---

### **Day 41 of #100DaysOfData Challenge**

I had to spend the day understanding and creating with other colleagues of the **Omdena** project, the mid-term document. We have completed half of the term for the project, 4 weeks, and we have to present to the partner our progress towards the planned derivables.

1. **Project**: [Building A Recommendation System For Cognitive Exercises and Training Programs](https://omdena.com/projects/cognitive-exercises/). My assigned task is more than half way completed, during these days I am trying to progress as much as possible to completed and be able to move to other tasks. Hopefully I can join some of the machine learning groups before finishing the project.

---

### **Day 31 to 40 of #100DaysOfData Challenge**

Unfortunatelly the holidays had come to its end, and now I am back to work. During this days I kept focusing on **Omdena**'s project and also had some time to progress in the course. 

1. **Project**: [Building A Recommendation System For Cognitive Exercises and Training Programs](https://omdena.com/projects/cognitive-exercises/). During these days we have had a lot of progress on data generation task. We have finalized the initial data sets generation, a total of 17 tables, and now we are moving to the next step. As a deliverable we have to create a synthetic database in MySQL, we have started to created the hosting (in AWS) and its structure. And now and for the next days we have to modify the code to populate the MySQL database instead of just output a CSV. I will take this opportunity to practice database creation and maintenance in MySQL.
2. **Course**: [2021 Python for Machine Learning & Data Science](https://www.udemy.com/course/python-for-machine-learning-data-science-masterclass/). During these 9 days I have completed the next modules:
   - Data cleaning adn feature enginering: The course goes over some of the most common techniques to deal with missing values, with outliers, normalize the values, etc.
   - Cross validation: In this module, the course introduces cross validation techniques using scikit learn. Even though it doesn't go through to the most up to date and best cross validation methods, it is more than enough to fully understand the purpose and the technique.

---

### **Day 30 of #100DaysOfData Challenge**

As I am still on holidays, my time for data science is reserved to **Omdena**'s project. If I can I will try to progress in other areas as well.

1. **Project**: [Building A Recommendation System For Cognitive Exercises and Training Programs](https://omdena.com/projects/cognitive-exercises/). Today was quite challenging to keep up with the feedback to our initial data sets generated. Organize the sub-tasks, give the feedback to the appropriate people takes most of the time. But actualy is the best way forwards, with this organization the team is confident to work in the same direction and the progress is better.

---

### **Day 23 to 29 of #100DaysOfData Challenge**

I have been on holidays from day 23 and I will still be until the day 39. But I kept doing at least something related to Data Science all these days, I have been working mainly in **Omdena**'s project to keep up with my commitment.

1. **Project**: [Building A Recommendation System For Cognitive Exercises and Training Programs](https://omdena.com/projects/cognitive-exercises/). During this week I have been focused organizing the task I have been assigned, to generate synthetic data. As it is the first step, we need to have an initial output to allow the rest of teams to start working. The challenging point is that there are many rules to comply but very little information from the company, but during the week we have improve a lot. Currently we have some initial data sets and outlined most of the rest of data sets. For next week we should complete all the outlines and have a data set for most.

---

### **Day 22 of #100DaysOfData Challenge**

Again not a coding day and also, not a great day for my data science progression. I am about to go on holidays and, even though I will keep working on **Omdena**'s project, I will stop almost anything else to enjoy a couple of weeks of holidays.

1. **Project**: [Building A Recommendation System For Cognitive Exercises and Training Programs](https://omdena.com/projects/cognitive-exercises/). After the new and better understanding of the problem to solve, we have decided to pivot the data acquisition task. The main reason is that we have determined that it is not possible to create a training data set because we do not have the list of items to recommend. Therefore, now the objective is generate a data set for testing and demo the API. It needs to be realistic and with some logic and limitations, but it does not need to be real. Tomorrow I will finish to draft the first data set and create the document to explain it.

---

### **Day 21 of #100DaysOfData Challenge**

Not a coding day, sometimes there is more progress when we don't touch any code... More discussions today to clarify and frame **Omdena**'s project and also some videos to complete linear regression module.

1. **Project**: [Building A Recommendation System For Cognitive Exercises and Training Programs](https://omdena.com/projects/cognitive-exercises/). The main activity today has been to prepare the meeting with NESTRE and then to try to extract as much information as we could. It is being really complicated to find the best way forwards, it feels that every time we have a meeting everything changes and we need to start over. But it feels we are closer now, tomorrow I will summarize everything and try to create sub-tasks for better organization.
2. **Course**: [2021 Python for Machine Learning & Data Science](https://www.udemy.com/course/python-for-machine-learning-data-science-masterclass/). Today I finished the linear regression module. Hopefully I can apply this new knowledge in my personal project soon. We have learn then next types of regression:
   - Univariate and multivariate simple regression: This is the most basic way to do regressions, just a line that tries to find the best fit for the targe variable using all the features.
   - Polynomial regression: In this regression type we increase the features by adding relations between features and higher grade features. Very interesting but dangerous to overfit.
   - Lasso, Ridge and Elastic Net regressions: To handle the overfitting problem we add a correction factor (depends on the type, we use a different type of factor or a combination) that will add noise, so we can handle better new data.

---

### **Day 20 of #100DaysOfData Challenge**

Another day focused on **Omdena**'s project; I know this weekend I will not be able to work as much, so I am trying to increase the work to meet the weekly hours I committed.

1. **Project**: [Building A Recommendation System For Cognitive Exercises and Training Programs](https://omdena.com/projects/cognitive-exercises/). Today I have finalized the EDA I was conducting in the Lumosity data set. I could find some interesting insight valuable when we have to prepare our own data set. I had some difficulties dealing with outliers and other modifications on a feature that contains a high level of granularity, for example, game results, including all game types. Tomorrow we have another Q&A meeting with Nestre to finalize framing the problem. Also I will summarize my findings on the EDA and post them to Slack.

---

### **Day 19 of #100DaysOfData Challenge**

Day fully focused on **Omdena**'s project, basically trying to perform EDA to the best data set we have found. I did not have time to spend in any other of my "tasks".

1. **Project**: [Building A Recommendation System For Cognitive Exercises and Training Programs](https://omdena.com/projects/cognitive-exercises/). Today I started again EDA activity on the Lumosity data set. In some way, it was good to lose the initial notebook as now I am doing it better from the beginning. Today I made the main DataFrame merging the three originals and dealt with missing values. Also, I found that the values of a feature I want to compare had loads of outliers, therefore I removed them. For tomorrow I want to normalize the values so they are comparable.

---

### **Day 18 of #100DaysOfData Challenge**

First contact with GitHub as a collaborator in a collective project. Today I had to learn how to create new branches and how to push my work without overwriting others work.

1. **Project**: [Building A Recommendation System For Cognitive Exercises and Training Programs](https://omdena.com/projects/cognitive-exercises/). Today I started the EDA on the only data set we could find. This is my first non-guided EDA, and I have to say that it is difficult to decide where and how to start. So far, it is a bit of a mess, but I could find some interesting points. The only drawback is that I have to correct it to allow other people to understand it. Also, as my first adventure in GitHub collaboration I deleted my own notebook, and unfortunately, I will need to start again tomorrow.

---

### **Day 17 of #100DaysOfData Challenge**

Thanks to communication, today, we improved our understanding of Nestre requirements. Communication is always a key in team working and more in inter-companies working. In addition, some more progress in house pricing prediction, a new method, less API intensive.

1. **Project**: [Zoopla London Houses Price Prediction from Scratch](https://github.com/diequies/zoopla_houses). The method I am using to get the closest tube station and the walking time between the station and the house is to select the two smallest euclidian distances (using coordinates) between the house coordinates and a list of all stations coordinates. Then to confirm the closest and find the walking time, I am using the [*MapBox API*](https://www.mapbox.com/), which is more dynamic and allows more and higher frequency requests.
2. **Project**: [Building A Recommendation System For Cognitive Exercises and Training Programs](https://omdena.com/projects/cognitive-exercises/). Today we had an insightful meeting, Nestre started to share a bit more information, which made the things clearer. Unfortunately, with this new information, the model and data set that we outlined changed slightly, so now we have to re-study. Also, I presented the task in front of the people, it was good, but I got no feedback.

---

### **Day 16 of #100DaysOfData Challenge**

Preparation day for the first weekly update, as a task leader I need to report what we have done so far. Also I am trying to find alternatives for my personal project, instead of relying too much on API such as [*Overpass Turbo API*](https://overpass-turbo.eu).

1. **Project**: [Zoopla London Houses Price Prediction from Scratch](https://github.com/diequies/zoopla_houses). Because of the limitation in requests of [*Overpass Turbo API*](https://overpass-turbo.eu), I need to find an alternative to be able to find the closest station and its walking distance in time for around 20k houses. My plan is instead of request the information regarding the closest train stations to the API a will create a list of the tube stations with the cordinates and pick the 2-3 closest for each house, and then request to an API this walking distance information using coordinates.
2. **Project**: [Building A Recommendation System For Cognitive Exercises and Training Programs](https://omdena.com/projects/cognitive-exercises/). Today I had to prepare the presentation of my task's progress, it is being quite difficult as the steps are really unclear and the progress has been very limited. But I will try to explain progress and challenges with the objective of increase our understanding with Nestre.

---

### **Day 15 of #100DaysOfData Challenge**

I have started to outline the expected data set for **Omdena**'s project and shared it with the team. So far, it is pretty disorienting, but I feel now that very slowly, things are getting more clear. Also, I keep progressing in my personal project of house pricing prediction.

1. **Project**: [Zoopla London Houses Price Prediction from Scratch](https://github.com/diequies/zoopla_houses). I have started to study the next step in this project, to calculate the walking time to go from the house to the closest station. I will use a different API that also connects with *OpenStreetMap*, called [*OpenStreetMap Routing Machine API*](http://project-osrm.org/). It looks quite straightforward; I have already started to review the documentation and almost have the expected output with a few lines of code. Tomorrow I plan to finalise this part of the code to identify the closest tube station and input the walking time.
2. **Project**: [Building A Recommendation System For Cognitive Exercises and Training Programs](https://omdena.com/projects/cognitive-exercises/). Today's main focus was to present the data set sample I outlined to the team and ask the questions that came up to Nestre. In addition, Nestre shared the wire-frame of their app to clarify a bit their requirements, and of course, it generated more questions to add. Based on their replies, I expect to progress tomorrow with the data set outline.
3. **Book**: [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/). During the last two days, I have finalised the *Numpy* chapter, and I have learned a lot, about the essential functions and some whys that are very insightful. However, at the end of the chapter, there are more complex functions and explanations that I found less useful, although it seems just the introduction to the next chapter about *Pandas*.
4. **Course**: [2021 Python for Machine Learning & Data Science](https://www.udemy.com/course/python-for-machine-learning-data-science-masterclass/). Today I have started the regression module, it is the longest one, so probably it will take some days to cover it. But, I am very interested in it because even though I have already studied them, my personal project will probably be the first time I put them into practice.

---

### **Day 14 of #100DaysOfData Challenge**

Another day spent on **Omdena**'s project, trying to put in order ideas. My main roadblock with the data-set acquisition is that I am not really sure of what comes next.

1. **Project**: [Building A Recommendation System For Cognitive Exercises and Training Programs](https://omdena.com/projects/cognitive-exercises/). My main task now is to acquire or generate a data-set as input for the models, or at least find the way. The initial steps are to outline the required data-set and find possible sources. We are seeing two fundamental roadblocks to accomplish those points, the lack of data and details from Nestre and my lack of skills and knowledge to understand what comes next. My plan for tomorrow is to ask my questions to Nestre and share my outline to get feedback.

---

### **Day 13 of #100DaysOfData Challenge**

Today I found difficult to spend time in my Data Science project of life but as a minimum I had 2h working on **Omdena** project and just a little bit on my house pricing project. 

1. **Project**: [Zoopla London Houses Price Prediction from Scratch](https://github.com/diequies/zoopla_houses). I am a little bit stuck with [*Overpass Turbo API*](https://overpass-turbo.eu), as it seems it is giving me problems because of too many attempts. I am trying different ways, like sleep time to wait between requests. Tomorrow I will keep trying other methods.
2. **Project**: [Building A Recommendation System For Cognitive Exercises and Training Programs](https://omdena.com/projects/cognitive-exercises/). Today we made a clear statement of the task we have been assigned, data-set acquisition, and specified the different methods we believe we can use to solve the task. While we wait for the data from the partner to decide the best approach, I will start looking for information and a baseline for each.

---

### **Day 12 of #100DaysOfData Challenge**

Quite an intense day today. We had our first team meeting with **Omdena**, I still feel pretty lost, but it is improving. Also, I made the first step forwards on Overpass Turbo API, although still far from what I want.

1. **Project**: [Zoopla London Houses Price Prediction from Scratch](https://github.com/diequies/zoopla_houses). Today, I had almost no time to invest in this project but, still, I have managed to make some progress using [*Overpass Turbo API*](https://overpass-turbo.eu). I can get now, for each house, using its coordinates, the tube stations within a specific range. The loop is relatively slow, and also I am still playing with the API timeouts. Currently, I am waiting 1 second per request, but I guess I will need to increase that. Once I get the stations related to each house, I have to find a way to estimate its walking distance and select the shortest one. I believe I won't be able to do it with Overpass Turbo API, but I am sure that there are resources out there to do it.
2. **Project**: [Building A Recommendation System For Cognitive Exercises and Training Programs](https://omdena.com/projects/cognitive-exercises/). Today, we had our first team meeting, and I wish I was more participative and that we could get more output from it. However I could improve my problem understanding and could suggest one task. This task is to create the dataset via survey, it is quite simple from a Data Science point of view, but it requires time and some research to avoid bias and get the required output. Tomorrow I will draft a path and some possible sub-tasks to propose.
3. **Book**: [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/). I keep going with the *NumPy* chapter, now it is going deeper, and it is showing how to do fancy indexing, binning or sorting using *NumPy*. Slow progress, but progress it is.
4. **Course**: [2021 Python for Machine Learning & Data Science](https://www.udemy.com/course/python-for-machine-learning-data-science-masterclass/). I completed the capstone exercise, where we had to use what we have learnt so far from *NumPy*, *Pandas*, *Matplotlib* and *Seaborn*. It was not so difficult but very useful to settle the previous work.

---

### **Day 11 of #100DaysOfData Challenge**

Another day of reading general Data Science articles suggested by **Omdena**, I am also trying to frame the problem myself, but I feel inexperience. I could start the second phase of my house pricing prediction project and continue reading and watching the course, quite a productive day.

1. **Project**: [Zoopla London Houses Price Prediction from Scratch](https://github.com/diequies/zoopla_houses). The objective of this new phase is to add more information and features related to the location of the house, such as walking distance to the closest tube station, the density of supermarkets/shops/restaurants, etc. To do so, I will use [*Overpass Turbo API*](https://overpass-turbo.eu), which is an open-source data filtering tool that uses OpenStreetMap data. So far, I am finding the query language a little bit complex, but I am now getting used. Tomorrow I want to extract at least the details of the tube stations in a range of 2km.
2. **Project**: [Building A Recommendation System For Cognitive Exercises and Training Programs](https://omdena.com/projects/cognitive-exercises/). Today, apart from keeping reading and trying to catch up on knowledge with my peers, I have started to frame the problem we need to solve. Tomorrow I will continue, and we will have a meeting to share our ideas and repeat the activity altogether again. The purpose will be to define the first tasks and hopefully some subtasks.
3. **Book**: [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/). I continued with the *NumPy* chapter, I am learning now the importance of vectorization and how it can improve the efficiency and speed of our code. Also, it explains the main aggregation and arithmetic functions, and the use of mask arrays.
4. **Course**: [2021 Python for Machine Learning & Data Science](https://www.udemy.com/course/python-for-machine-learning-data-science-masterclass/). After some days not being able to watch more videos of the course, I managed to finish the *seaborn* library module. Nothing entirely new, but quite useful to better understand how it works and some not straightforward functions. The following module is a capstone project for visualization to check what we have learnt so far.

---

### **Day 10 of #100DaysOfData Challenge**

Even though I was focused today to gain knowledge for **Omdena**'s project, I also managed to finalise Zoopla data extraction on my personal project for house pricing prediction. Additionally, as every day, I spend some time reading the book.

1. **Project**: [Zoopla London Houses Price Prediction from Scratch](https://github.com/diequies/zoopla_houses). I have finalised data extraction and processing from Zoopla. It has been very tedious, mainly because web scrapping was very slow; I was scrapping information from more than 50k houses (and therefore webpages). Also, the quality of the data encoded was relatively low, and I had to complement it using complex pattern extraction. I am sure that with NLP techniques, it would be much easier. The next step is to add information about the location of the house. I am thinking of something like the closest station, supermarket, etc. and the distance in time.
2. **Project**: [Building A Recommendation System For Cognitive Exercises and Training Programs](https://omdena.com/projects/cognitive-exercises/). Today I have been reviewing articles and papers provided by **Omdena** and some shared by the team collaborators. I could also find an interesting survey paper ([link](https://arxiv.org/pdf/2101.06286.pdf)) summarizing some techniques for recommender systems using reinforcement learning and deep learning. So far, it is out of my capabilities, but I found it interesting, and the team might be able to extract values from it. I would be delighted to learn how to implement this kind of model during the project. Tomorrow we have our first meeting to frame the problem, and hopefully, the first task will flourish.
3. **Book**: [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/). I continued with the *NumPy* chapter, basically the fundamental operations and why they are different from normal Python. Also, it is reviewing the *ufuncs*, which gives me a good idea of *NumPy* capabilities and a reference to use in my code.

---

### **Day 9 of #100DaysOfData Challenge**

On the first day being part of **Omdena**'s project, I have spent most of the time reading and learning as basically it is my first time in almost all the facets of the project. Also, I kept working on my personal project and reading the book slowly but steady.

1. **Project**: [Zoopla London Houses Price Prediction from Scratch](https://github.com/diequies/zoopla_houses). Once I cleared my issues with pattern extraction, I have been able to progress at a good pace. Today I have completed the feature extraction from text features and almost finished cleaning and filling missing values. My plan for tomorrow is to finish the dataset and create the SQL database, which won't be easy as I have never done it.
2. **Project**: [Building A Recommendation System For Cognitive Exercises and Training Programs](https://omdena.com/projects/cognitive-exercises/) On the first day of the project, I explored and read all I could from **Omdena**'s internal resources, mainly Git procedures and other articles for data flow and treatment. Also, I have read many articles and papers for recommendation systems, but as it is the first time I face this sort of project, I still need to learn a lot. So my plan for tomorrow is to finish with the internal resources, keep learning about recommendation systems and find any valuable resource that I can share with the team. So far, I am finding quite challenging the interaction and the way of working, but I am sure that soon I will get used to and be able to collaborate more and better.
3. **Book**: [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/): Today I started the *NumPy* chapter, and again, as it starts from the basics, I am learning a lot. Also, I am understanding now some issues I was having because I did not have the knowledge adequately settled. Today's readings were focused on why *NumPy* is faster than normal *Python* and the data types handled as a *NumPy* array.

---

### **Day 8 of #100DaysOfData Challenge**

Today I had the kick-off meeting of the new project I am involved in, with [**Omdena**](https://omdena.com/). For those who do not know what **Omdena** is, it is an *NGO* focus on *AI* and *ML* for good. Apart from that, I could work a little bit on my personal project.

1. **Project**: [Zoopla London Houses Price Prediction from Scratch](https://github.com/diequies/zoopla_houses). The little time I had today for this project, I reviewed the data I currently had cleaned it. Remove or complete missing values, remove useless or already used features, etc. For tomorrow I hope I can find some time to start extracting various features from *features* and *description* texts, such as parking, tenure, etc.
2. **Project**: [Building A Recommendation System For Cognitive Exercises and Training Programs](https://omdena.com/projects/cognitive-exercises/) Today was the meeting with the company with which we will work to understand better the goals and requirements of the project or product. The project aims to develop a machine learning algorithm that recommends cognitive exercises and training to people in mental treatment to improve their condition. One of the keys is that it must take into account implicit feedback from the user. So far, I have started by reading different papers and resources suggested as my knowledge is really limited in this field. Also, I am very interested in the way *Omdena* works, with huge freedom and a completely horizontal hierarchy.

---

### **Day 7 of #100DaysOfData Challenge**

After loads of effort and incredibly huge learning, not only on pattern extraction but also about flow and data types, I managed to get the best possible quantity of floor area records. I spent almost the day finalizing the code, but I could read something at the end of the day.

1. **Project**: [Zoopla London Houses Price Prediction from Scratch](https://github.com/diequies/zoopla_houses). Today, I improved yesterday's mark of 26% to 40% of floor area feature records. It means that I have around 20k houses with floor area information out of 50k houses. However, I don't think it is enough to fill the missing values, so that I will drop the NaN values rows. Also, a sample of 20k houses with a total population of 50k seems enough to make reliable predictions. The next step is to finalize the feature extraction from text features and clean the data before storing it in a SQL server.
2. **Book**: [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/): I couldn't read as much as I wish, but at least I finished *IPython* chapter, being surprisingly didactic. From tomorrow, I will start reading the *NumPy* chapter, and hopefully I can grasp deep knowledge on this library.

---

### **Day 6 of #100DaysOfData Challenge**

Yesterday I was so focused on improve pattern extraction that I couldn't write here how was the day. Finally, I could improve the results, but there is still a long way. I could also spend some time with the course and the book.

1. **Project**: [Zoopla London Houses Price Prediction from Scratch](https://github.com/diequies/zoopla_houses). As I said, I could improve the floor area extraction, but it is still around 25% of records with information. To be able to work with this dataset, I need to improve such an important feature.
2. **Course**: [2021 Python for Machine Learning & Data Science](https://www.udemy.com/course/python-for-machine-learning-data-science-masterclass/). I also finalized the *Seaborn* section, and hopefully, I can practice what I learnt soon by doing some exploratory analysis in my project.
3. **Book**: [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/): Sadly, I spent the least time was reading the book. I could just keep reading the IPython chapter, in this case about time measurement and profiling.

---

### **Day 5 of #100DaysOfData Challenge**

Finally, I beat the pattern extraction issue I was having, and it has been thanks to some essential debugging tips I learnt from the book. However, today most than any, I feel how important it is to settle down the learnings.

1. **Project**: [Zoopla London Houses Price Prediction from Scratch](https://github.com/diequies/zoopla_houses). I managed to find my issues with pattern extraction, mainly inappropriate use of data flow and data types. With these, I take two main learnings: first, taking a step back sometimes is more effective than keeping fighting, and second, debugging practices are essential and a weakness I detected and need to improve. The next step will be to improve the patterns list to extract floor area data as current performance is relatively poor.
2. **Course**: [2021 Python for Machine Learning & Data Science](https://www.udemy.com/course/python-for-machine-learning-data-science-masterclass/). Today, I finished the *Matplotlib* section, even though I had already experience using it, it was good to improve my understanding of the OOP way to use it. The next one will be the *Seaborn* library.
3. **Book**: [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/): I am currently reading the first chapter, which speaks extensively about IPython. Even though I was already using Jupytern Notebooks that runs with IPython, I didn't know 99% of its features. Today, it was especially useful the debugging one.

---

### **Day 4 of #100DaysOfData Challenge**

Most of the day, I have been battling to extract floor area from the *"description"* and *"features"* texts; after that, I finalised the *Pandas* section of the course and finally started another book.

1. **Project**: [Zoopla London Houses Price Prediction from Scratch](https://github.com/diequies/zoopla_houses). As I said, I spent most of the day trying to extract floor data from text features. I have found many issues, mainly due to data types and data flow, when I wanted to solve them. I think I am overcomplicating the functions. Therefore my plan for tomorrow is to start from scratch the functions with what I learned in the first trial loop.
2. **Course**: [2021 Python for Machine Learning & Data Science](https://www.udemy.com/course/python-for-machine-learning-data-science-masterclass/). Today, I finished *Pandas* section of the course, including the Notebook with exercises. So far, I am finding the course quite complete and with a good combination of theory and practice, of course, if you have the will to complete the exercises.
3. **Book**: [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/): Finally, today I have started another Python book, the aim is to settle down and complete my Python for Data Science before start going deeper in Machine Learning. I have begun the *Chapter 1*, which is about IPython and gives a basic overview of its added functions to normal Python.
 
---

### **Day 3 of #100DaysOfData Challenge**

Today I spent most of the time on the London Houses project as the extraction of floor area information is quite tricky. Also, I kept watching some videos from the course.

1. **Project**: [Zoopla London Houses Price Prediction from Scratch](https://github.com/diequies/zoopla_houses). The main point I have been working on today has been the pattern extraction for the floor area I started yesterday. First, I reviewed and improved the custom functions I built, basically improving the efficiency and dealing with some possible problems it may raise in the future. And second, to try to adapt the functions to work more generally and apply to the *"description"* as well as the *"features"*. I expect to finalize the floor area extraction tomorrow and then decide how to deal with the missing values.
2. **Course**: [2021 Python for Machine Learning & Data Science](https://www.udemy.com/course/python-for-machine-learning-data-science-masterclass/). Today I couldn't progress that much in the course. In this case, I continued watching the *Pandas* section, talking about string, time and date methods and how to deal with these data types in a DataFrame.

---

### **Day 2 of #100DaysOfData Challenge**

Today wasn't the most productive day because I went to get my second Covid-19 jab. But anyway, I managed to spend some time on my house price project and watch some more videos.

1. **Project**: [Zoopla London Houses Price Prediction from Scratch](https://github.com/diequies/zoopla_houses). Floor area is, in theory, one of the most important features when it comes to determining a house price, but I found that from scrapping features directly from JSON code, I could get around a 20% of the records. With such a small amount, I would very likely drop this feature and lose loads of information. Currently, I am trying to extract floor area information from *"features"* and *"description"* features using pattern recognition. So far, I have succeeded using *"features"*, and the previous 20% turns to 26%. The next step will be to use the *"description"*, which might be more challenging because it is less structured.
2. **Course**: [2021 Python for Machine Learning & Data Science](https://www.udemy.com/course/python-for-machine-learning-data-science-masterclass/). From the course, I watched more videos of the *Pandas* section. Specifically to cover the following functions:
    - **Groupby**: Function to aggregate or combine specific rows or columns, where it is also possible to apply statistical functions, like mean, count, etc. It is handy to summarize a DataFrame using specific values and features.
    - **Cross Section**: This function is related to filter or select data action. It is helpful to use it when the DataFrame has more than one level of index.
    - **Aggregate**: Similar to groupby function but with the extra functionality of passing a dictionary with the statistical operations to apply in specific features.
    - **Concatenate and Merge**: These functions are the most commonly used to join DataFrames with *Pandas*, concatenate basically will join the rows or columns filling the non-common values with NaNs and merge gives you much more options of the type of join we want to perform (inner, left, right or outer) and which column use as joining index.

---

### **Day 1 of #100DaysOfData Challenge**

**Challenge Presentation**: I am starting a repository to track my first #100DaysOfData challenge. Currently, I am looking to have a change in my career from Mechanical Engineer to Data Scientist. So far, I have taken several courses, read books and tried exercises and the famous [Titanic Competition](https://github.com/diequies/Titanic_Kaggle) from Kaggle. But from now and for the next 100 days, I will start recording all the work I do to become a Data Scientist.

I will establish some rules to follow this challenge to make sure I can have the best output:
1. To do something related to Data Science every single day. As a minimum, read some pages of a book, an article, whatever but something.
2. To keep a record of everything I do, then I can go back to check what I have done and review my performance or effectiveness. Also, it would be beneficial if someone arrives at this repository as a summary of resources.
3. Try always to keep activated the three main items, a project, a book/paper and a course or certification.

To start the challenge, I currently have the following activities ongoing:
1. **Project**: [Zoopla London Houses Price Prediction from Scratch](https://github.com/diequies/zoopla_houses). Where I am trying to perform as a primary objective a price prediction on London Houses prices with information scrapped from Zoopla. As a side aim, I want to add as much information as possible from several sources, like crime indexes, distances to tube, etc. and produce some clustering output.
2. **Course**: [2021 Python for Machine Learning & Data Science](https://www.udemy.com/course/python-for-machine-learning-data-science-masterclass/). I have found excellent critiques about this Udemy course. Therefore I am taking it as a review and to settle my current knowledge in Python and Data Science tools. I have already started the course and going through the Pandas module at the moment.
3. **Book**: [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/): I haven't started yet to read it, but it seems a good book to, again, settle the knowledge of Python and Data Science I have learnt so far. Also it will be helpful as a reference book.
